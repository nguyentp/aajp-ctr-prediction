{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys, collections\n",
    "from datetime import datetime\n",
    "from utils.common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAIN_PATH = \"../tr.r0.csv\"\n",
    "RAW_VAL_PATH = \"../va.r0.csv\"\n",
    "\n",
    "# file name after pre-processing step (processed csv file)\n",
    "PROCESSED_TRAIN_PATH = \"processed_train.csv\"\n",
    "PROCESSED_VAL_PATH = \"processed_val.csv\"\n",
    "\n",
    "# hashed csv file's name\n",
    "HASHED_TRAIN_PATH = \"hashed_train.csv\"\n",
    "HASHED_VAL_PATH = \"hashed_val.csv\"\n",
    "\n",
    "ROWS_FOR_TRAINING = -1 # set as -1 if you want to train with entire date\n",
    "LEARNING_RATE = 0.03\n",
    "EPOCHS = 13\n",
    "\n",
    "# removed 'pub_id', 'pub_domain', 'pub_category', added 'app/site_id', 'app/site_domain', 'app/site_category'\n",
    "FIELDS = ['id','click','hour','banner_pos','device_id','device_ip','device_model','device_conn_type','C14','C17','C20','C21',\n",
    "         'app_id', 'app_domain', 'app_category', 'site_id', 'site_domain', 'site_category']\n",
    "NEW_FIELDS = FIELDS+['device_id_count','device_ip_count','user_count','smooth_user_hour_count','user_click_histroy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate counting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(path, is_train):\n",
    "    '''\n",
    "    copy from base/util/gen_data.py\n",
    "    '''\n",
    "    for i, row in enumerate(csv.DictReader(open(path)), start=1):\n",
    "        if i >= ROWS_FOR_TRAINING and is_train:\n",
    "            break\n",
    "        user = def_user(row)\n",
    "        id_cnt[row['device_id']] += 1\n",
    "        ip_cnt[row['device_ip']] += 1\n",
    "        user_cnt[user] += 1\n",
    "        user_hour_cnt[user+'-'+row['hour']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('Start: {0}'.format(str(start)))\n",
    "\n",
    "id_cnt = collections.defaultdict(int)\n",
    "ip_cnt = collections.defaultdict(int)\n",
    "user_cnt = collections.defaultdict(int)\n",
    "user_hour_cnt = collections.defaultdict(int)\n",
    "\n",
    "scan(RAW_TRAIN_PATH, is_train=ROWS_FOR_TRAINING != -1)\n",
    "scan(RAW_VAL_PATH, False)\n",
    "print('End: {0}, Elapsed time: {1}'.format(\n",
    "        str(datetime.now()),\n",
    "        str(datetime.now() - start))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add counting features & history features to new csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(src_path, dst_path, is_train):\n",
    "    '''\n",
    "    copy from base/util/gen_data.py\n",
    "    '''\n",
    "    reader = csv.DictReader(open(src_path))\n",
    "    writer = csv.DictWriter(open(dst_path, 'w'), NEW_FIELDS)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, row in enumerate(reader, start=1):\n",
    "        if i >= ROWS_FOR_TRAINING and is_train:\n",
    "            break\n",
    "        new_row = {}\n",
    "        for field in FIELDS:\n",
    "            new_row[field] = row[field]\n",
    "\n",
    "        new_row['device_id_count'] = id_cnt[row['device_id']]\n",
    "        new_row['device_ip_count'] = ip_cnt[row['device_ip']]\n",
    "\n",
    "        user, hour = def_user(row), row['hour']\n",
    "        new_row['user_count'] = user_cnt[user]\n",
    "        new_row['smooth_user_hour_count'] = str(user_hour_cnt[user+'-'+hour])\n",
    "\n",
    "        if has_id_info(row):\n",
    "\n",
    "            if history[user]['prev_hour'] != row['hour']:\n",
    "                history[user]['history'] = (history[user]['history'] + history[user]['buffer'])[-4:]\n",
    "                history[user]['buffer'] = ''\n",
    "                history[user]['prev_hour'] = row['hour']\n",
    "\n",
    "            new_row['user_click_histroy'] = history[user]['history']\n",
    "\n",
    "            if is_train:\n",
    "                history[user]['buffer'] += row['click']\n",
    "        else:\n",
    "            new_row['user_click_histroy'] = ''\n",
    "            \n",
    "        writer.writerow(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('Start: {0}'.format(str(start)))\n",
    "\n",
    "history = collections.defaultdict(lambda: {'history': '', 'buffer': '', 'prev_hour': ''})\n",
    "\n",
    "gen_data(src_path=RAW_TRAIN_PATH, dst_path=PROCESSED_TRAIN_PATH,is_train=ROWS_FOR_TRAINING != -1)\n",
    "gen_data(src_path=RAW_VAL_PATH, dst_path=PROCESSED_VAL_PATH, is_train=False)\n",
    "\n",
    "print('End: {0}, Elapsed time: {1}'.format(\n",
    "        str(datetime.now()),\n",
    "        str(datetime.now() - start))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralized hashing PROCESSED_TRAIN/VAL_PATH and save to HASHED_TRAIN/VAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_thread = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split processed file into nr_thread csv_files\n",
    "split(path=PROCESSED_TRAIN_PATH, nr_thread=nr_thread)\n",
    "split(path=PROCESSED_VAL_PATH, nr_thread=nr_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelly hashing splited csv_files and save to nr_thread hashed csv_files\n",
    "parallel_convert(\n",
    "    \"utils/2.py\", \n",
    "    [PROCESSED_TRAIN_PATH, PROCESSED_VAL_PATH, HASHED_TRAIN_PATH, HASHED_VAL_PATH], \n",
    "    nr_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old splited processed files\n",
    "delete(PROCESSED_TRAIN_PATH, nr_thread)\n",
    "delete(PROCESSED_VAL_PATH, nr_thread)\n",
    "\n",
    "# merge nr_thread hashed csv_files into 1 file\n",
    "cat(HASHED_TRAIN_PATH, nr_thread)\n",
    "cat(HASHED_VAL_PATH, nr_thread)\n",
    "\n",
    "# delete old splited hashed csv_files\n",
    "delete(HASHED_TRAIN_PATH, nr_thread)\n",
    "delete(HASHED_VAL_PATH, nr_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_thread = 1 # number of thread for training\n",
    "cmd = '../base/mark1 -r {0} -s {1} -t {2} {3} {4}'.format(LEARNING_RATE, no_thread, EPOCHS, HASHED_VAL_PATH, HASHED_TRAIN_PATH)\n",
    "subprocess.call(cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
